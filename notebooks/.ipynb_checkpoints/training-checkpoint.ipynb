{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965b6d54",
   "metadata": {},
   "source": [
    "# Durak Training Notebook with Live Plotting\n",
    "\n",
    "This notebook demonstrates how to train your AlphaZero-like model for Durak while showing live updates of both training loss and evaluation win rate in real-time.\n",
    "\n",
    "## Requirements\n",
    "1. You must have the updated code in `src/model/trainer.py` that implements:\n",
    "   - Reward shaping\n",
    "   - Forced game-length limit\n",
    "   - Mixed training vs. rule agent\n",
    "   - Imitation learning methods (optional)\n",
    "2. You must have a rule-based agent in `src/evaluation/rule_agent.py` and an evaluator in `src/evaluation/evaluator.py`.\n",
    "3. This notebook will import and use those modules.\n",
    "4. Install `matplotlib`, `ipywidgets`, and `ipython` if you want interactive plotting.\n",
    "\n",
    "## Usage\n",
    "You can run this entire notebook cell-by-cell. It will:\n",
    "1. Import needed modules.\n",
    "2. Optionally do imitation learning from a rule-based agent.\n",
    "3. Run a custom training loop with dynamic plotting.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d90a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ivan/Projects/Neurodurak/AlphaZero-Durak\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "\n",
    "# Set up environment\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import pyspiel  # Make sure open_spiel is installed\n",
    "from src.durak.durak_game import DurakGame\n",
    "from src.model.network import AlphaZeroNet\n",
    "from src.model.trainer import Trainer\n",
    "from src.evaluation.evaluator import evaluate_model_vs_rule_agent\n",
    "from src.evaluation.rule_agent import RuleAgent\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9126f2b0",
   "metadata": {},
   "source": [
    "## Initialize the Game and Network\n",
    "\n",
    "Here we create a bigger network (256 hidden dims, 4 layers) to match the advanced approach.\n",
    "Adjust `hidden_dim`, `num_layers`, or other parameters as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae75fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DurakGame()\n",
    "network = AlphaZeroNet(\n",
    "    input_dim=158,  # from DurakObserver\n",
    "    hidden_dim=256, # bigger net\n",
    "    num_actions=40, # 36 card plays + 4 extra actions\n",
    "    num_layers=4    # deeper\n",
    ").to(device)\n",
    "\n",
    "# Create the advanced trainer. Adjust parameters as you like:\n",
    "trainer = Trainer(\n",
    "    network=network,\n",
    "    game=game,\n",
    "    device=device,\n",
    "    learning_rate=1e-3,\n",
    "    mcts_simulations=200,  # increase for better lookahead\n",
    "    c_puct=1.0,\n",
    "    temperature=1.0,       # can lower to reduce randomness\n",
    "    use_argmax=False,      # can switch to True after warm-up\n",
    "    take_cards_penalty=0.5,\n",
    "    move_penalty=0.01,\n",
    "    max_moves=40,          # forcibly end after 40 moves\n",
    "    forced_terminal_reward=-1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f709f",
   "metadata": {},
   "source": [
    "## Warm Start: Imitation Learning\n",
    "Here we can train the model to imitate the rule agent for a certain number of games, e.g. 500 or 1000, to avoid fringe behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e870f355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating rule agent dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating rule agent dataset: 100%|█████████| 500/500 [00:01<00:00, 371.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 34813\n",
      "Imitation learning from rule agent...\n",
      "Epoch 1/3, Supervised Loss: 1.6077\n",
      "Epoch 2/3, Supervised Loss: 1.2605\n",
      "Epoch 3/3, Supervised Loss: 1.0493\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating rule agent dataset...\")\n",
    "dataset = trainer.generate_rule_agent_dataset_for_imitation(n_games=500)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "print(\"Imitation learning from rule agent...\")\n",
    "trainer.train_supervised_on_rule_agent_dataset(dataset, batch_size=64, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69273f38",
   "metadata": {},
   "source": [
    "## Live Training Loop With Dynamic Plotting\n",
    "Below, we run a custom loop (similar to `run_training_with_mixed_opponents` or `run_training`) but in a manual Python `for` loop so we can update the plots after **each** iteration.\n",
    "\n",
    "We'll:\n",
    "1. Mix some fraction of games vs. the rule agent, and some self-play.\n",
    "2. Collect training data, train in batches.\n",
    "3. Evaluate vs. the rule agent.\n",
    "4. Plot the results in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "note-reward-shaping",
   "metadata": {},
   "source": [
    "Note: The trainer now incorporates improved reward shaping so that winning actions are rewarded appropriately while moves like 'take cards' carry a heavier penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e879cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# model vs rule agent\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_vs_rule):\n\u001b[0;32m---> 23\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_vs_rule_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_player\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_player\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     iteration_data\u001b[38;5;241m.\u001b[39mextend(data)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# self-play\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Neurodurak/AlphaZero-Durak/src/model/trainer.py:213\u001b[0m, in \u001b[0;36mTrainer.play_vs_rule_agent\u001b[0;34m(self, model_player)\u001b[0m\n\u001b[1;32m    211\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(policy\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     actions, probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    214\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(actions, p\u001b[38;5;241m=\u001b[39mprobs)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Store the training example\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for training\n",
    "num_iterations = 250      # total training iterations\n",
    "games_per_iteration = 10 # total games each iteration\n",
    "fraction_vs_rule = 0.5   # fraction of games vs rule agent\n",
    "batch_size = 32\n",
    "eval_interval = 5        # evaluate every 5 iterations\n",
    "eval_games = 20          # number of eval games vs. rule agent\n",
    "model_player = 0         # model is player 0, rule agent is player 1\n",
    "\n",
    "# Storage for plotting\n",
    "loss_history = []\n",
    "winrate_history = []\n",
    "eval_iterations = []\n",
    "\n",
    "for it in range(num_iterations):\n",
    "    # 1) Gather training data\n",
    "    iteration_data = []\n",
    "    num_vs_rule = int(games_per_iteration * fraction_vs_rule)\n",
    "    num_self = games_per_iteration - num_vs_rule\n",
    "\n",
    "    # model vs rule agent\n",
    "    for _ in range(num_vs_rule):\n",
    "        data = trainer.play_vs_rule_agent(model_player=model_player)\n",
    "        iteration_data.extend(data)\n",
    "\n",
    "    # self-play\n",
    "    for _ in range(num_self):\n",
    "        data = trainer.self_play_game()\n",
    "        iteration_data.extend(data)\n",
    "\n",
    "    random.shuffle(iteration_data)\n",
    "\n",
    "    # 2) Train\n",
    "    losses = []\n",
    "    for i in range(0, len(iteration_data), batch_size):\n",
    "        batch = iteration_data[i:i+batch_size]\n",
    "        loss_val, _, _ = trainer.train_step(batch)\n",
    "        losses.append(loss_val)\n",
    "    avg_loss = np.mean(losses) if losses else 0\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    # 3) Evaluate every eval_interval\n",
    "    if (it+1) % eval_interval == 0:\n",
    "        win_rate = evaluate_model_vs_rule_agent(\n",
    "            network=trainer.network,\n",
    "            device=trainer.device,\n",
    "            num_games=eval_games,\n",
    "            model_player=model_player,\n",
    "            mcts_simulations=trainer.mcts_simulations,\n",
    "            temperature=trainer.temperature,\n",
    "            use_argmax=True  # deterministic for eval\n",
    "        )\n",
    "        winrate_history.append(win_rate)\n",
    "        eval_iterations.append(it+1)\n",
    "        print(f\"Iteration {it+1}/{num_iterations}, Loss: {avg_loss:.4f}, Win vs rule: {win_rate*100:.1f}%\")\n",
    "    else:\n",
    "        print(f\"Iteration {it+1}/{num_iterations}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 4) Live plot with combined loss and win rate on same figure\n",
    "    clear_output(wait=True)\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot loss on left y-axis\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Training Loss', color='blue')\n",
    "    ax1.plot(range(1, len(loss_history)+1), loss_history, 'bo-', label='Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Create a second y-axis for win rate\n",
    "    ax2 = ax1.twinx()  \n",
    "    ax2.set_ylabel('Win Percentage', color='red')\n",
    "    if winrate_history:  # Only plot if we have data\n",
    "        ax2.plot(eval_iterations, [w*100 for w in winrate_history], 'ro-', label='Win %')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    ax2.set_ylim(0, 100)  # 0-100%\n",
    "\n",
    "    # Add a title\n",
    "    plt.title('AlphaZero Durak Training Progress')\n",
    "\n",
    "    # Add a legend\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 5) Save checkpoint\n",
    "    from src.utils.checkpoint import save_checkpoint\n",
    "    save_checkpoint(trainer.network, trainer.game_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a8500",
   "metadata": {},
   "source": [
    "## Results\n",
    "When training completes, the final plots remain displayed in the last cell. You can also re-plot them by referencing `loss_history`, `winrate_history`, and `eval_iterations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb6ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to plot them again:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1, len(loss_history)+1), loss_history, marker='o', label='Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "if winrate_history:\n",
    "    plt.plot(eval_iterations, [w*100 for w in winrate_history], marker='o', label='Win %')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Win Percentage')\n",
    "plt.title('Evaluation Win % vs. Rule Agent')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
